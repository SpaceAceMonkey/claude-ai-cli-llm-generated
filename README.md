# Claude AI CLI (LLM Generated)

![Claude AI CLI Terminal Interface](claude_command-line_tool4b.png)

**All code and text in this repository, including this README, was written by AI language models.**

## About This Project

This is a terminal-based client for interacting with Claude AI via the Anthropic API. The entire codebase was generated through conversations with various AI language models, with only minimal human intervention for file creation and git operations.

### Human Contribution

The only code written by a human in this repository consists of:
- One tiny change early in development: replacing `.as_str()` with `.trim()` 
- Manual copy/pasting of AI-generated code into files when diff features failed
- Creating new files when AI models couldn't
- All git commits and commit messages

### Screenshots

The screenshots in this README file were captured by a human user and are not AI-generated content. These images document the actual appearance and functionality of the AI-generated application.

### Disclaimer

**No human has reviewed or verified the accuracy of the claims made in this README.** All descriptions of features, challenges, development history, and insights were generated by AI language models and should be understood in that context. However, the screenshots in this README were captured by a human user and document the actual appearance and functionality of the AI-generated application.

## Features

### Core Functionality
- **Claude API Integration** - Send messages to Claude AI and display responses
- **Simulate Mode** (`--simulate` flag) - Test the UI without making real API calls
- **Token Tracking** - Real-time display of input/output/total token usage
- **Full Conversation Context** - Maintains complete conversation history for API calls

### User Interface
- **Three-Panel Layout**:
  - **Top Panel**: Conversation history with syntax highlighting
  - **Middle Panel**: Multi-line input box with word wrapping
  - **Bottom Panel**: Status bar (left) and token usage (right)

- **Advanced Navigation**:
  - Auto-scroll to latest messages (configurable)
  - PageUp/PageDown for manual scrolling
  - Manual scrolling disables auto-scroll; scrolling to bottom re-enables
  - Input box auto-scrolls to keep cursor visible

- **Input Features**:
  - Multi-line input with proper word wrapping
  - Full cursor positioning with arrow keys
  - Home/End key support
  - Backspace/Delete functionality
  - Command history navigation (Up/Down arrows)

### Configurable Behaviors

- **SHIFT_ENTER_SENDS** flag (currently false):
  - When false: Enter sends, Shift/Alt+Enter inserts newline, Ctrl+Enter sends
  - When true: Enter inserts newline, Shift/Alt+Enter sends, Ctrl+Enter sends

- **Auto-scroll Flags**:
  - `SCROLL_ON_USER_INPUT` - Auto-scroll when user sends message
  - `SCROLL_ON_API_RESPONSE` - Auto-scroll when Claude responds

- **Progress Animation** - Animated dots while waiting for API response

## Usage

### Command Line Arguments

```bash
claudecli --api-key <API_KEY> [OPTIONS]
```

Options:
- `--api-key <API_KEY>` - Your Anthropic API key (required)
- `--model <MODEL>` - AI model to use (default: claude-3-opus-20240229)
- `--max-tokens <MAX_TOKENS>` - Maximum tokens in response (default: 4096)
- `--temperature <TEMPERATURE>` - Response randomness 0.0-1.0 (default: 0.7)
- `--simulate` - Run in simulate mode (no API calls)

### Example Invocations

**Real API Usage:**
```bash
# Basic usage with API key
claudecli --api-key sk-ant-api03-xxxxx

# With custom model and parameters
claudecli --api-key sk-ant-api03-xxxxx --model claude-3-sonnet-20240229 --max-tokens 2048 --temperature 0.5
```

**Simulated Mode (for testing):**
```bash
# Test the UI without making API calls
claudecli --api-key dummy-key --simulate

# Simulate with different parameters
claudecli --api-key dummy-key --simulate --max-tokens 1024 --temperature 0.9
```

### Sample Output

```
┌─ Conversation ──────────────────────────────────────────┐
│[user]: Write a haiku about the Rust programming language│
│[assistant]: Here's a haiku about Rust:                  │
│[assistant]:                                             │
│[assistant]: Memory safe code                            │
│[assistant]: No null or dangling points                  │
│[assistant]: Compiler guides me                          │
│[user]: Another, please.                                 │
│[assistant]: Here's another Rust haiku:                  │
│[assistant]:                                             │
│[assistant]: Ownership rules strict                      │
│[assistant]: Borrowing keeps data safe                   │
│[assistant]: Fast as bare metal                          │
└─────────────────────────────────────────────────────────┘
┌─ Input (Ctrl+Enter to send, Shift/Alt+Enter for newline)┐
│                                                         │
└─────────────────────────────────────────────────────────┘
┌─ Status ─────────────────┬─ Token Usage ────────────────┐
│Ready                     │Input tokens: 127, Output    │
│                          │tokens: 89, Total tokens: 216│
└──────────────────────────┴──────────────────────────────┘
```

## Development Challenges

### Module Organization
- Started with a 2400+ line `main.rs` with 6+ duplicate API call blocks
- Refactored into proper modules (api, client, handlers, utils, config)
- Complex import management and module declarations

### Terminal Input Handling
- **Shift+Enter Detection**: Many terminal emulators don't pass through Shift+Enter
- Added Alt+Enter as alternative
- Different terminals handle modifiers inconsistently

### Message Display Issues
- **Duplication Bug**: Quick successive inputs caused messages to appear, disappear, then reappear
- Root cause: Entire message list was being replaced when API responses arrived
- Solution: Append only new assistant messages

### Scroll Calculation Complexity
- Long/wrapped messages were initially cut off
- Required calculating visual lines after text wrapping
- Complex interaction between logical lines and visual lines

### Async Communication
- Initial refactoring broke API integration
- Implemented proper channel (mpsc) between spawned tasks and main loop
- Ensuring UI updates correctly with async responses

### Feature Preservation
- PageUp/PageDown scrolling was repeatedly lost during refactoring
- Progress animation stopped working for real API calls
- Features had to be re-implemented multiple times across different AI models

## AI Model Development History

### ChatGPT 4.1
- Created the initial working version
- Eventually got stuck in a loop trying to re-implement PageUp/PageDown functionality
- Had previously implemented this feature successfully but removed it during other changes

### Claude 4 Sonnet
- More capable than ChatGPT 4.1 but inherited a more complex codebase
- Fell into a "fixing problem one causes problem two" loop
- Used for the shortest duration of the three models

### Claude 4 Opus
- Most capable model overall
- Created fewest bugs, especially during refactoring
- Made one significant mess of syntax errors that took considerable time to fix
- Like all models, frequently deleted previously working functionality
- PageUp/PageDown functionality had to be re-implemented approximately a dozen times across all models

## Development Insights

### Human Guidance Required
Despite no code being written by humans, significant guidance was necessary:
- Questions had to be carefully phrased to lead models to working solutions
- An inexperienced programmer could not have achieved similar results
- Total development time: One Sunday afternoon/evening plus Monday from 1:00 PM onward

### AI Strengths
The most valuable AI contribution was **library and crate discovery**:
- Discovered CLAP and Structopt for command-line argument processing
- Suggested appropriate crates for specific functionality
- This knowledge transfer is where AI models excel

### AI Weaknesses
All models exhibited similar problems:
- **Poor at refactoring**: Breaking code into separate files was consistently problematic
- **Feature deletion**: All models repeatedly removed working functionality
- **Cost vs. Capability**: Claude 4 Opus was ~4x more capable but 10x theoretically (30x actually) more expensive

### Cost Analysis
- Sonnet (1x multiplier): Used only 1.5% of credits over extended period
- Opus (10x multiplier): Burned through 38% of monthly credits in a few hours
- Total usage: ~40% of monthly allotment, almost entirely from Opus despite joining the project late

## Current TODOs
1. **Message Queueing** - Queue user inputs and interleave with responses for better conversation flow
2. **Clipboard Support** - Ctrl+V handling is currently stubbed out
3. **Debug Messages** - Show debug info in chat area with special formatting

---

*This entire project demonstrates both the capabilities and limitations of current AI language models in software development. While they can generate functional code with proper guidance, they require experienced human oversight to achieve production-quality results.*

## Installation

### Installing Rust

Install Rust using rustup:
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.cargo/env
```

For more details, visit [rustup.rs](https://rustup.rs/).

### Clone this repository

```bash
git clone https://github.com/SpaceAceMonkey/claude-ai-cli-llm-generated.git
cd claude-ai-cli-llm-generated
```

### Running with Cargo

You can run the application directly using `cargo run`:

**Real API Usage:**
```bash
# Basic usage with API key
cargo run -- --api-key sk-ant-api03-xxxxx

# With custom model and parameters
cargo run -- --api-key sk-ant-api03-xxxxx --model claude-3-sonnet-20240229 --max-tokens 2048 --temperature 0.5
```

**Simulated Mode (for testing):**
```bash
# Test the UI without making API calls
cargo run -- --api-key dummy-key --simulate

# Simulate with different parameters
cargo run -- --api-key dummy-key --simulate --max-tokens 1024 --temperature 0.9
```